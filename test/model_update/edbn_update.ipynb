{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append('/Users/meruozhu/Downloads/MP_data/MP_codes/MP')\n",
    "import sys\n",
    "sys.path.append('/Users/meruozhu/Downloads/MP_data/DynaTrainCDD/DynaTrainCDD')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import percentileofscore\n",
    "from Data.data import Data\n",
    "from edbn.Utils.LogFile import LogFile\n",
    "import edbn.Predictions.setting as setting\n",
    "from edbn import Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dynamiccdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/meruozhu/Downloads/MP_data/MP_codes/MP')\n",
    "import sys\n",
    "sys.path.append('/Users/meruozhu/Downloads/MP_data/DynaTrainCDD/DynaTrainCDD')\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import time\n",
    "import pandas as pd\n",
    "from scipy.stats import percentileofscore\n",
    "from Data.data import Data\n",
    "from edbn.Utils.LogFile import LogFile\n",
    "import edbn.Predictions.setting as setting\n",
    "from edbn import Methods\n",
    "from collections import deque\n",
    "from itertools import islice\n",
    "from collections import OrderedDict\n",
    "from PrefixTreeCDD.PrefixTreeClass import PrefixTree\n",
    "import PrefixTreeCDD.settings as settings\n",
    "from PrefixTreeCDD.CDD import Window\n",
    "from skmultiflow.drift_detection import ADWIN, PageHinkley\n",
    "import math\n",
    "from numpy import log as ln\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "\n",
    "def recCurve(a, b, x):\n",
    "    return a * math.log(x+1, 10)**(b * math.log(x+12, 10)) + 5000\n",
    "\n",
    "\n",
    "def recCurve2(x):\n",
    "    return -2e-5*x**2 + 1000\n",
    "\n",
    "\n",
    "def recCurveFinal(x,a,u_max):\n",
    "    if x == 0:\n",
    "        return 1\n",
    "    return 1 - math.exp(a*(x/u_max-1))\n",
    "\n",
    "\n",
    "def store_results(file, results):\n",
    "    with open(file, \"w\") as fout:\n",
    "        for r in results:\n",
    "            fout.write(\",\".join([str(r_i) for r_i in r]) + \"\\n\")\n",
    "\n",
    "\n",
    "def store_resultsa(file, results):\n",
    "    with open(file, \"a\") as fout:\n",
    "        for r in results:\n",
    "            fout.write(\",\".join([str(r_i) for r_i in r]) + \"\\n\")\n",
    "\n",
    "\n",
    "def store_timings(file, timings):\n",
    "    with open(file, \"w\") as fout:\n",
    "        for t in timings:\n",
    "            fout.write(str(t) + \"\\n\")\n",
    "\n",
    "\n",
    "def store_timing(file, timing):\n",
    "    with open(file, \"w\") as fout:\n",
    "        fout.write(str(timing) + \"\\n\")\n",
    "\n",
    "\n",
    "def store_timinga(file, timing):\n",
    "    with open(file, \"a\") as fout:\n",
    "        fout.write(str(timing) + \"\\n\")\n",
    "\n",
    "\n",
    "def run(config, file, dataName, updateInt, updateWindow, trainPerc=0):\n",
    "\n",
    "    # df = pd.DataFrame(batch_accuracy)\n",
    "    datetime_str= datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    # Define the folder to save the CSV files\n",
    "    output_folder_results =Path(__file__).parent/\"results\"/f\"{Path(config.get_dataset()).stem}_rbs{config.get_recent_buffer_size()}_hbs{config.get_hard_buffer_size()}_{datetime_str}\"\n",
    "    if output_folder_results.exists():\n",
    "        print(f\"Folder {output_folder_results} already exists. Results will be overwritten.\")\n",
    "    output_folder_results.mkdir(parents=True, exist_ok=True)\n",
    "    print(output_folder_results)\n",
    "\n",
    "    #a = 0.1\n",
    "    delay_val = 490 #MUST BE LESS THAN U_MAX! has been 1250\n",
    "    u_min = 20 #has been 200\n",
    "    u_max = 75 #has been 1500\n",
    "    t_min = 20 #has been 500\n",
    "    t_max = 75 #has been 750\n",
    "    # u_min = 20 #has been 200\n",
    "    # u_max = 50 #has been 1500\n",
    "    # t_min = 50 #has been 500\n",
    "    # t_max = 75 #has been 750\n",
    "    a = ln(0.2) / ((delay_val /u_max) - 1)\n",
    "    updateArray = range(u_min, u_max)\n",
    "    trainArray = range(t_max, t_min, -1)\n",
    "    print('updateArray', updateArray, 'trainArray', trainArray)\n",
    "    file = file\n",
    "    data = pd.read_csv(file, low_memory=False)\n",
    "    timeformat = \"%Y-%m-%d %H:%M:%S\"\n",
    "    numEvents = data.shape[0]\n",
    "    print(\"Num events is {}\".format(numEvents))\n",
    "\n",
    "    d = Data(dataName,\n",
    "             LogFile(filename=file, delim=\",\", header=0, rows=None, time_attr=\"completeTime\", trace_attr=\"case\",\n",
    "                     activity_attr='event', convert=False))\n",
    "    d.logfile.keep_attributes(['event', 'role', 'completeTime'])\n",
    "    m = Methods.get_prediction_method(\"SDL\")\n",
    "    s = setting.STANDARD\n",
    "    trainPerc = trainPerc\n",
    "    s.train_percentage = trainPerc * 100\n",
    "    # # #\n",
    "    d.prepare(s)\n",
    "    # d.create_batch(\"normal\", timeformat)\n",
    "    is_written = 0\n",
    "    print(\"Test Context Data\")\n",
    "    print(d.test_orig.contextdata)\n",
    "    basic_model = m.train(d.train,{\"epochs\": 0, \"early_stop\": 10})\n",
    "\n",
    "    #print(\"Runtime %s:\" % m, time.time() - start_time)\n",
    "    res = m.test(basic_model, d.test_orig)\n",
    "    \n",
    "    store_results(str(output_folder_results)+\"/%s_%s_normal.csv\" % (m.name, d.name), res)\n",
    "    \n",
    "    connect_symbol=\"-\"\n",
    "    if '/' in d.logfile.get_data()['completeTime'][0]:\n",
    "        connect_symbol='/'\n",
    "    formats = [f\"%Y{connect_symbol}%m{connect_symbol}%d %H:%M:%S%z\", f'%Y{connect_symbol}%m{connect_symbol}%d %H:%M:%S',f'%Y{connect_symbol}%m{connect_symbol}%d %H:%M:%S.%f']\n",
    "\n",
    "    for timeformat in formats:\n",
    "        try:\n",
    "            pd.to_datetime(d.logfile.get_data()['completeTime'], format=timeformat,exact=True)\n",
    "            print(d.logfile.get_data()['completeTime'])\n",
    "            print(\"The time format is:\", timeformat)\n",
    "            break\n",
    "        except ValueError:\n",
    "            continue\n",
    "    print('timeformat',timeformat)\n",
    "    d.add_data_to_test_orig([17.5,50],timeformat,config.get_ratio())\n",
    "\n",
    "    print(\"Baseline Complete\")\n",
    "    updateInt = updateInt\n",
    "    updateWindow = updateWindow\n",
    "    window_size = 10\n",
    "    tree_size = 1000\n",
    "    decay_lambda = 0.25\n",
    "    noise = 1\n",
    "    settings.init()\n",
    "    endEventsDic = dict()\n",
    "    window = Window(initWinSize=window_size)\n",
    "\n",
    "    lastEvents = data.groupby(['case']).last()\n",
    "    for _, row in lastEvents.iterrows():\n",
    "        endEventsDic[_] = [str(row['event']), row['completeTime']]\n",
    "\n",
    "    caseList = []  # Complete list of cases seen\n",
    "    Dcase = OrderedDict()  # Dictionary of cases that we're tracking.\n",
    "    print(\"You are here\")\n",
    "    tree = PrefixTree(pruningSteps=tree_size, noiseFilter=noise,\n",
    "                      lambdaDecay=decay_lambda)  # Create the prefix tree with the first main node empty\n",
    "    adwin = ADWIN()\n",
    "    ph = PageHinkley()\n",
    "\n",
    "    pruningCounter = 0  # Counter to check if pruning needs to be done\n",
    "    traceCounter = 0  # Counter to create the Heuristics Miner model\n",
    "\n",
    "    eventCounter = 0  # Counter for number of events\n",
    "    currentNode = tree.root  # Start from the root node\n",
    "    testInd = round(numEvents * trainPerc)\n",
    "    prevDriftCounter = 0\n",
    "    drifts = {}\n",
    "    new_drifts = {}\n",
    "    severity = -88\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _, event in data.iterrows():\n",
    "        caseList, Dcase, currentNode, pruningCounter, traceCounter, window = tree.insertByEvent(caseList, Dcase,\n",
    "                                                                                                currentNode, event,\n",
    "                                                                                                pruningCounter,\n",
    "                                                                                                traceCounter,\n",
    "                                                                                                endEventsDic, window)\n",
    "        eventCounter += 1\n",
    "\n",
    "        if window.cddFlag:  # If a complete new tree has been created\n",
    "            if len(window.prefixTreeList) == window.WinSize:\n",
    "                # Maximum size of window reached, start concept drift detection within the window\n",
    "                temp_drifts = window.conceptDriftDetection(adwin, ph, eventCounter)\n",
    "                window.WinSize = min(window.WinSize + 1, window.maxWindowSize)\n",
    "                for i in temp_drifts.keys():\n",
    "                    if i not in drifts.keys():\n",
    "                        new_drifts[i] = temp_drifts[i]\n",
    "\n",
    "                if len(window.prefixTreeList) == window.WinSize:  # If there was no drift detected within the window\n",
    "                    window.prefixTreeList = deque(islice(window.prefixTreeList, 1, None))  # Drop the oldest tree\n",
    "\n",
    "        #TODO: when drift occurs or other trigger is activated\n",
    "        if len(list(drifts.keys()) + list(new_drifts.keys())) > prevDriftCounter and (_ > round(numEvents * trainPerc)):\n",
    "            if len(list(drifts.keys())) >= 5:\n",
    "                print('Take quantile of any new drifts that occur')\n",
    "                #TODO: Take quantile of any new drifts that occur\n",
    "                #list of drift severities\n",
    "                drift_sevs = [drifts[i]['treeDist'] for i in list(drifts.keys())]\n",
    "                new_drift_sevs = [new_drifts[i]['treeDist'] for i in list(new_drifts.keys())]\n",
    "                severity = percentileofscore(a = drift_sevs, score = sum(new_drift_sevs)/len(new_drift_sevs))/100 #* 5000\n",
    "                print(\"Severity is {}\".format(severity))\n",
    "                #calculate new severity using percentilesum(\n",
    "\n",
    "            else:\n",
    "                #TODO: max severity level\n",
    "                severity = 1\n",
    "                print(\"Max Severity level\")\n",
    "        elif (_ - testInd > updateInt) and (_ > round(numEvents * trainPerc)):\n",
    "            #TODO: last drift greater than predetermined retrain frequency -> assign moderate severity\n",
    "            print('last drift greater than predetermined retrain frequency -> assign moderate severity')\n",
    "            if severity == -1 or severity == -88:\n",
    "                print(\"Max update interval level\")\n",
    "                severity = 0.5\n",
    "        elif severity == -88 and (_ > round(numEvents * trainPerc)) and len(list(drifts.keys()))>0:# last drift was < 5000 events ago....then update with the most recent available information\n",
    "            print('last drift was < 5000 events ago....then update with the most recent available information')\n",
    "            if _ - sorted([i for i in drifts.keys()])[-1] < 5000:\n",
    "                severity = 1\n",
    "\n",
    "\n",
    "        #TODO: become aware of drift severity above then make appropriate update below^^^\n",
    "        #TODO: drift updating starts here, use this to set the updateInt from previous iterations\n",
    "        if severity != -1 and severity != -88 and ((_ - testInd > updateInt) or len(list(new_drifts.keys())) > 0):\n",
    "            severity = min(1, severity)\n",
    "            print('severity',severity)\n",
    "\n",
    "            #TODO: Update these values and create a list using rec_curve_final\n",
    "            #severity outputs a score between 0 and 1\n",
    "            #need to use the update and train array to actually convert to a meaningful update/train value\n",
    "            print(\"Update val index value is {}\".format(round(recCurveFinal(severity, a, u_max) * len(updateArray)) - 1))\n",
    "            updateVal = min(updateArray[round(recCurveFinal(severity, a, u_max) * len(updateArray)) - 1], u_max)\n",
    "            winVal = max(t_min, trainArray[round(recCurveFinal(severity, a, u_max) * len(trainArray)) - 1])\n",
    "            updateVal, winVal = 49, 50\n",
    "            updateInt, updateWindow = updateVal, winVal\n",
    "            print(\"Update Value is {}\".format(updateVal))\n",
    "            print(\"Window Value is {}\".format(winVal))\n",
    "            #TODO: determine new values for updateInt and updateWindow before running below\n",
    "            print(\"Performing Maintenance Update\")\n",
    "            print(\"Size of hyphen is {}\".format(_))\n",
    "            print(\"Highest index calculation on test is {}\".format(_ - round(numEvents * trainPerc)))\n",
    "            if updateWindow > _ - round(numEvents * trainPerc):\n",
    "                print('updateWindow > _ - round(numEvents * trainPerc)')\n",
    "                updateWindow = 0\n",
    "            result, timing, basic_model = m.test_and_update_indices(basic_model, d, (_ - round(numEvents * trainPerc),\n",
    "                                                                                     max(_ - round(\n",
    "                                                                                         numEvents * trainPerc) - updateWindow, #maximally the total number in the test set to retrain\n",
    "                                                                                         0)),\n",
    "                                                                    testInd - round(numEvents * trainPerc),\n",
    "                                                                    _ - round(numEvents * trainPerc),\n",
    "                                                                    reset=False)\n",
    "            testInd = _\n",
    "            print(\"Model updated at event {}\".format(str(_)))\n",
    "            if is_written:\n",
    "                store_resultsa(str(output_folder_results)+\"/%s_%s_OTF_drift_%s.csv\" % (m.name, d.name, 'dynamic'), result)\n",
    "                store_timinga(str(output_folder_results/(\"%s_%s_OTF_drift_%s_time.csv\" % (m.name, d.name, 'dynamic'))),\n",
    "                              timing)\n",
    "            else:\n",
    "                store_results(str(output_folder_results)+\"/%s_%s_OTF_drift_%s.csv\" % (m.name, d.name, 'dynamic'), result)\n",
    "                store_timing(str(output_folder_results/(\"%s_%s_OTF_drift_%s_time.csv\" % (m.name, d.name, 'dynamic'))),\n",
    "                             timing)\n",
    "            is_written = 1\n",
    "            #severity += 0.1\n",
    "\n",
    "        #reset drifts for next run's analysis\n",
    "        drifts = {**drifts, **new_drifts}\n",
    "        prevDriftCounter += len(list(new_drifts.keys()))\n",
    "        new_drifts = {}\n",
    "        if severity >= 1:\n",
    "            severity = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print(\"Size of hyphen is {}\".format(_))\n",
    "    # print(\"Highest index calculation on test is {}\".format(_ - round(numEvents * trainPerc)))\n",
    "    if _!=testInd:\n",
    "        print(\"Performing Final Test\")\n",
    "        result, timing, basic_model = m.test_and_update_indices(basic_model, d, (_ - round(numEvents * trainPerc),\n",
    "                                                                                max(_ - round(\n",
    "                                                                                    numEvents * trainPerc) - updateWindow,\n",
    "                                                                                    0)),\n",
    "                                                                testInd - round(numEvents * trainPerc),\n",
    "                                                                _ - round(numEvents * trainPerc),\n",
    "                                                                reset=False)\n",
    "        testInd = _\n",
    "\n",
    "        print(\"Model updated at event {}\".format(str(_)))\n",
    "        if is_written:\n",
    "            store_resultsa(str(output_folder_results/(\"%s_%s_OTF_drift_%s.csv\" % (m.name, d.name, 'dynamic'))), result)\n",
    "            store_timinga(str(output_folder_results/(\"%s_%s_OTF_drift_%s_time.csv\" % (m.name, d.name, 'dynamic'))), timing)\n",
    "        else:\n",
    "            store_results(str(output_folder_results/(\"%s_%s_OTF_drift_%s.csv\" % (m.name, d.name, 'dynamic'))), result)\n",
    "            store_timing(str(output_folder_results/(\"%s_%s_OTF_drift_%s_time.csv\" % (m.name, d.name, 'dynamic'))), timing)\n",
    "\n",
    "    prediction_results_path = str(output_folder_results)+\"/%s_%s_OTF_drift_%s.csv\" % (m.name, d.name, 'dynamic')\n",
    "    timing_path = str(output_folder_results/(\"%s_%s_OTF_drift_%s_time.csv\" % (m.name, d.name, 'dynamic')))\n",
    "    end_time = time.time()\n",
    "    running_time = end_time - start_time\n",
    "    print(\"Running time is {}\".format(running_time))\n",
    "    return output_folder_results, prediction_results_path, timing_path,running_time\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     intervals = [1500] #has been 1500\n",
    "#     windows = [750] #has been 750\n",
    "#     files = ['/Users/meruozhu/Downloads/MP_data/MP_codes/MP/Data/BPIC15_1_sorted_new.csv']#['Data/BPIC17_FULL.csv']#['Data/BPIC15_1_sorted_new.csv', 'Data/BPIC15_2_sorted_new.csv', 'Data/BPIC15_3_sorted_new.csv', 'Data/BPIC15_4_sorted_new.csv', 'Data/BPIC15_5_sorted_new.csv',\n",
    "#              # 'Data/Helpdesk.csv', 'Data/BPIC11.csv', 'Data/BPIC12.csv']\n",
    "\n",
    "#     #names = ['BPIC15_1_OTF_OPT']\n",
    "#     names = ['BPIC17FULL'] #, 'BPIC15_2_OTF_OPT', 'BPIC15_3_OTF_OPT', 'BPIC15_4_OTF_OPT', 'BPIC15_5_OTF_OPT',\n",
    "#               # 'Helpdesk_OTF_OPT', 'BPIC11_OTF_OPT', 'BPIC12_OTF_OPT']\n",
    "#     for i in intervals:\n",
    "#         for w in windows:\n",
    "#             for f in range(len(files)):\n",
    "#                 if i < 1000 and w < 1000:\n",
    "#                     run(files[f], names[f], i, w)\n",
    "#                 elif i >= 1000:\n",
    "#                     run(files[f], names[f], i, w)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num events is 300\n",
      "PREPARE\n",
      "CONVERT\n",
      "PREPROCESSING: Converting event\n",
      "PREPROCESSING: Converting role\n",
      "PREPROCESSING: Converting case\n",
      "K-CONTEXT\n",
      "Create k-context: 10\n",
      "SPLIT TRAIN-TEST\n",
      "Train: 0\n",
      "Test: 300\n",
      "Test Context Data\n",
      "     level_0  index  event_Prev9  role_Prev9  completeTime_Prev9  case_Prev9  \\\n",
      "0          0    191            0           0                   0          59   \n",
      "1          1    143            0           0                   0          43   \n",
      "2          2    234            0           0                   0          70   \n",
      "3          3     58            0           0                   0          17   \n",
      "4          4    220            0           0                   0          67   \n",
      "..       ...    ...          ...         ...                 ...         ...   \n",
      "295      295    225            0           0                   0          68   \n",
      "296      296    226            0           0                   0          68   \n",
      "297      297    167            0           0                   0          50   \n",
      "298      298    168            0           0                   0          50   \n",
      "299      299    181            0           0                   0          55   \n",
      "\n",
      "     event_Prev8  role_Prev8  completeTime_Prev8  case_Prev8  ...  \\\n",
      "0              0           0                   0          59  ...   \n",
      "1              0           0                   0          43  ...   \n",
      "2              0           0                   0          70  ...   \n",
      "3              0           0                   0          17  ...   \n",
      "4              0           0                   0          67  ...   \n",
      "..           ...         ...                 ...         ...  ...   \n",
      "295            0           0                   0          68  ...   \n",
      "296            0           0                   0          68  ...   \n",
      "297            0           0                   0          50  ...   \n",
      "298            0           0                   0          50  ...   \n",
      "299            0           0                   0          55  ...   \n",
      "\n",
      "     completeTime_Prev1  case_Prev1  event_Prev0  role_Prev0  \\\n",
      "0                     0          59            0           0   \n",
      "1                     0          43            0           0   \n",
      "2                     0          70            0           0   \n",
      "3                     0          17            0           0   \n",
      "4                     0          67            0           0   \n",
      "..                  ...         ...          ...         ...   \n",
      "295                   0          68            0           0   \n",
      "296                   0          68            1           6   \n",
      "297                   0          50            0           0   \n",
      "298                   0          50            1           6   \n",
      "299                   0          55            0           0   \n",
      "\n",
      "          completeTime_Prev0  case_Prev0 event  role             completeTime  \\\n",
      "0                          0          59     1     6  2010/01/13 07:40:25.000   \n",
      "1                          0          43     1     6  2010/01/13 11:26:04.000   \n",
      "2                          0          70     1     6  2010/01/13 11:30:37.000   \n",
      "3                          0          17     1     6  2010/01/13 12:09:31.000   \n",
      "4                          0          67     1     6  2010/01/13 16:25:25.000   \n",
      "..                       ...         ...   ...   ...                      ...   \n",
      "295                        0          68     1     6  2010/03/09 07:06:03.000   \n",
      "296  2010/03/09 07:06:03.000          68     6     6  2010/03/09 07:06:12.000   \n",
      "297                        0          50     1     6  2010/03/09 07:18:19.000   \n",
      "298  2010/03/09 07:18:19.000          50     1     6  2010/03/09 07:18:29.000   \n",
      "299                        0          55     1     2  2010/03/09 08:08:53.000   \n",
      "\n",
      "     case  \n",
      "0      59  \n",
      "1      43  \n",
      "2      70  \n",
      "3      17  \n",
      "4      67  \n",
      "..    ...  \n",
      "295    68  \n",
      "296    68  \n",
      "297    50  \n",
      "298    50  \n",
      "299    55  \n",
      "\n",
      "[300 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "file = '/Users/meruozhu/Downloads/MP_data/MP_codes/MP/Data/test_dataset/Helpdesk_mini.csv'\n",
    "data = pd.read_csv(file, low_memory=False)\n",
    "timeformat = \"%Y-%m-%d %H:%M:%S\"\n",
    "numEvents = data.shape[0]\n",
    "print(\"Num events is {}\".format(numEvents))\n",
    "\n",
    "d = Data('Helpdesk_mini',\n",
    "            LogFile(filename=file, delim=\",\", header=0, rows=None, time_attr=\"completeTime\", trace_attr=\"case\",\n",
    "                    activity_attr='event', convert=False))\n",
    "d.logfile.keep_attributes(['event', 'role', 'completeTime'])\n",
    "m = Methods.get_prediction_method(\"SDL\")\n",
    "s = setting.STANDARD\n",
    "trainPerc = 0\n",
    "s.train_percentage = trainPerc * 100\n",
    "# # #\n",
    "d.prepare(s)\n",
    "# d.create_batch(\"normal\", timeformat)\n",
    "is_written = 0\n",
    "print(\"Test Context Data\")\n",
    "print(d.test_orig.contextdata)\n",
    "basic_model = m.train(d.train,{\"epochs\": 0, \"early_stop\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp_model/assets\n",
      "Accuracy: 62.00%\n",
      "train data: evWindow[1]:  90 evWindow[0]:  150 test data: prevTestInd:  50 currentTestInd:  150\n",
      "num_batches: 3\n",
      "===========================================\n",
      "Batch: 0\n",
      "===========================================\n",
      "Batch: 1\n",
      "===========================================\n",
      "Batch: 2\n"
     ]
    }
   ],
   "source": [
    "batch_inputs_lst,batch_expected_lst = m.test_and_update_indices(basic_model,d,(150,90),50,150,reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp_model/assets\n",
      "Accuracy: 62.00%\n",
      "train data: evWindow[1]:  90 evWindow[0]:  150 test data: prevTestInd:  50 currentTestInd:  150\n"
     ]
    }
   ],
   "source": [
    "results, timings, model=m.test_and_update_indices(basic_model,d,(150,90),50,150,reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7f92f9892370>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 16)\n",
      "(20, 16)\n",
      "(20, 28)\n"
     ]
    }
   ],
   "source": [
    "for item in batch_inputs_lst: print(np.shape(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 20 (16,) [0 1 6 0 1 6 7 0 0 1 6 1 0 1 6 0]\n"
     ]
    }
   ],
   "source": [
    "print(type(batch_inputs),len(batch_inputs),np.shape(batch_inputs[0]),batch_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(batch_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred tf.Tensor(\n",
      "[[0.11963906 0.13621168 0.11888967 0.12126113 0.12076897 0.12262568\n",
      "  0.13337098 0.12723288]\n",
      " [0.12007792 0.13274083 0.11783526 0.1209626  0.12197915 0.12245729\n",
      "  0.13532451 0.12862247]\n",
      " [0.12012846 0.13331342 0.11801156 0.12121197 0.11964029 0.12327831\n",
      "  0.1338708  0.1305452 ]\n",
      " [0.11930355 0.136541   0.11919087 0.11994054 0.12247207 0.12241466\n",
      "  0.13331364 0.12682365]\n",
      " [0.12145756 0.13264136 0.11858703 0.12044381 0.11977818 0.12176019\n",
      "  0.13643503 0.12889685]\n",
      " [0.11976922 0.13337825 0.11768399 0.12158215 0.11966705 0.12335523\n",
      "  0.13357565 0.13098851]\n",
      " [0.11933742 0.1319848  0.11801521 0.1216635  0.12029705 0.12323031\n",
      "  0.13352202 0.13194966]\n",
      " [0.1193942  0.13705905 0.11819307 0.12220512 0.12096544 0.12310756\n",
      "  0.13243523 0.12664041]\n",
      " [0.119739   0.13690358 0.11862326 0.12057627 0.12019862 0.12287108\n",
      "  0.13329177 0.12779647]\n",
      " [0.11981081 0.1325371  0.11929288 0.12035406 0.12071384 0.12282973\n",
      "  0.13630348 0.1281581 ]\n",
      " [0.11984017 0.13323446 0.11752784 0.12179445 0.12071843 0.12418238\n",
      "  0.1330098  0.1296924 ]\n",
      " [0.11935894 0.13306743 0.11874806 0.12116207 0.12061866 0.12267575\n",
      "  0.13672017 0.1276489 ]\n",
      " [0.11969297 0.13705471 0.11953667 0.12085166 0.12103134 0.12297843\n",
      "  0.13291894 0.1259353 ]\n",
      " [0.11977474 0.13371454 0.11895846 0.12133682 0.12094683 0.12186738\n",
      "  0.13621461 0.12718667]\n",
      " [0.11924961 0.13265009 0.11846011 0.1230627  0.12100265 0.1238129\n",
      "  0.13292496 0.12883702]\n",
      " [0.11973791 0.13560534 0.11819208 0.12105359 0.12168638 0.12255493\n",
      "  0.13333489 0.12783483]], shape=(16, 8), dtype=float32) y_sup [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]] tf.Tensor(\n",
      "[0.10661013 0.10682976 0.10802585 0.10652855 0.10655581 0.10791579\n",
      " 0.10767344 0.10639885 0.10644083 0.10658646 0.11020959 0.10648492\n",
      " 0.10639932 0.10661009 0.1097009  0.10676073], shape=(16,), dtype=float32) total_loss tf.Tensor(1.715731, shape=(), dtype=float32)\n",
      "y_pred tf.Tensor(\n",
      "[[0.11798123 0.13974655 0.11637326 0.11976757 0.11993409 0.12260786\n",
      "  0.13584062 0.12774883]\n",
      " [0.11799942 0.13607524 0.11607271 0.119235   0.12070183 0.12041289\n",
      "  0.13910905 0.13039383]\n",
      " [0.11812551 0.13375357 0.11695219 0.12181279 0.11981351 0.12288668\n",
      "  0.13524067 0.13141504]\n",
      " [0.11788119 0.14005826 0.11693777 0.11971545 0.11931605 0.12133247\n",
      "  0.13619563 0.12856326]\n",
      " [0.11819424 0.1353409  0.11704968 0.11864024 0.12072695 0.12104347\n",
      "  0.13832077 0.13068372]\n",
      " [0.11815291 0.13544258 0.11541885 0.12014531 0.11900146 0.12275976\n",
      "  0.13626531 0.13281386]\n",
      " [0.11853129 0.13433711 0.11680336 0.12037303 0.11985365 0.12143045\n",
      "  0.1348931  0.133778  ]\n",
      " [0.11790516 0.14048867 0.11814266 0.11988897 0.1188715  0.1220683\n",
      "  0.13539332 0.12724137]\n",
      " [0.11818612 0.14048594 0.11732277 0.11902508 0.11920407 0.1211094\n",
      "  0.13683102 0.12783566]\n",
      " [0.11838189 0.13459632 0.11716235 0.12044468 0.11905456 0.12287577\n",
      "  0.139347   0.12813741]\n",
      " [0.11877259 0.13452108 0.11569707 0.12053705 0.11998638 0.12352669\n",
      "  0.13628675 0.13067241]\n",
      " [0.1175483  0.13705306 0.11683156 0.11948013 0.11828686 0.12230926\n",
      "  0.13974932 0.12874149]\n",
      " [0.11735187 0.13998212 0.11710143 0.11944734 0.11988773 0.12230922\n",
      "  0.13635246 0.12756784]\n",
      " [0.11710616 0.13627052 0.11617728 0.12009795 0.11932388 0.12196672\n",
      "  0.13919969 0.12985773]\n",
      " [0.11875726 0.13437033 0.11492628 0.12133442 0.12024566 0.12415184\n",
      "  0.13488705 0.13132714]\n",
      " [0.11787804 0.14068362 0.11656814 0.12008806 0.11927337 0.12242259\n",
      "  0.13642037 0.1266658 ]], shape=(16, 8), dtype=float32) y_sup [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]] tf.Tensor(\n",
      "[0.10575399 0.10591678 0.10781827 0.1056797  0.10610738 0.10748406\n",
      " 0.10723447 0.10556815 0.10557573 0.10584744 0.11054359 0.10576041\n",
      " 0.1056976  0.10589481 0.10963739 0.10552466], shape=(16,), dtype=float32) total_loss tf.Tensor(1.7060444, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "for gs in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = basic_model([tf.convert_to_tensor(np.asarray(row).reshape(-1,1)) for row in batch_inputs],training=True)\n",
    "        y_sup = batch_expected\n",
    "        recent_loss = loss_fn(y_sup, y_pred)\n",
    "        total_loss = tf.reduce_sum(recent_loss)\n",
    "        gradients = tape.gradient(total_loss, basic_model.trainable_variables)\n",
    "        print('y_pred',y_pred,'y_sup',y_sup,recent_loss,'total_loss',total_loss)\n",
    "        basic_model.optimizer.apply_gradients(zip(gradients, basic_model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basic_model\n",
    "loss_fn = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.engine.training.Model'>\n",
      "<class 'list'>\n",
      "[array([0, 1, 6, 0, 1, 6, 7, 0, 0, 1, 6, 1, 0, 1, 6, 0]), array([0, 0, 1, 0, 0, 1, 6, 0, 0, 0, 1, 0, 0, 0, 1, 0]), array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 5, 5, 0, 5, 5, 5, 0, 0, 6, 6, 6, 0, 6, 6, 0]), array([0, 0, 5, 0, 0, 5, 5, 0, 0, 0, 6, 0, 0, 0, 6, 0]), array([0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
      "batch_expected <class 'numpy.ndarray'> [[0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(type(batch_inputs))\n",
    "print(batch_inputs)\n",
    "print('batch_expected',type(batch_expected),batch_expected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gs in range(10):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model([tf.convert_to_tensor(np.asarray(row).reshape(-1,1)) for row in batch_inputs],training=True)\n",
    "        #print(type(y_pred),y_pred)\n",
    "        y_sup = batch_expected\n",
    "        #print(type(y_sup),y_sup)\n",
    "        recent_loss = loss_fn(y_sup, y_pred)\n",
    "        total_loss = tf.reduce_sum(recent_loss)\n",
    "        gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "        model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = [tf.convert_to_tensor(np.asarray(row).reshape(-1,1)) for row in batch_inputs][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp_model/assets\n",
      "Accuracy: 61.00%\n",
      "train data: evWindow[1]:  90 evWindow[0]:  150 test data: prevTestInd:  50 currentTestInd:  150\n",
      "num_batches: 3\n",
      "===========================================\n",
      "Batch: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find matching function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * Tensor(\"inputs:0\", shape=(16, 1), dtype=int64)\n  Keyword arguments: {}\n\nExpected these arguments to match one of the following 1 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')\n  Keyword arguments: {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m_/vqx5l_qd50xbdyfwyvj3dnf40000gn/T/ipykernel_7186/2550103049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_and_update_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbasic_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Downloads/MP_data/MP_codes/MP/edbn/Methods/method.py\u001b[0m in \u001b[0;36mtest_and_update_indices\u001b[0;34m(self, model, data, evWindow, prevTestInd, currentTestInd, reset)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#     model = self.update(model, train_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_expected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/MP_data/MP_codes/MP/edbn/Methods/method.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, model, train_data, params)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/MP_data/MP_codes/MP/edbn/Methods/SDL/sdl.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(model, log)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0my_sup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_expected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mrecent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_sup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    715\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    716\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    821\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 822\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/utils.py\u001b[0m in \u001b[0;36mreturn_outputs_and_add_losses\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_arg_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_arg_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/pmGrad/lib/python3.7/site-packages/tensorflow_core/python/saved_model/function_deserialization.py\u001b[0m in \u001b[0;36mrestored_function_body\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         .format(_pretty_format_positional(args), kwargs,\n\u001b[1;32m    261\u001b[0m                 \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcrete_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 \"\\n\\n\".join(signature_descriptions)))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mconcrete_function_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find matching function to call loaded from the SavedModel. Got:\n  Positional arguments (1 total):\n    * Tensor(\"inputs:0\", shape=(16, 1), dtype=int64)\n  Keyword arguments: {}\n\nExpected these arguments to match one of the following 1 option(s):\n\nOption 1:\n  Positional arguments (1 total):\n    * TensorSpec(shape=(None, 1), dtype=tf.float32, name='inputs')\n  Keyword arguments: {}"
     ]
    }
   ],
   "source": [
    "m.test_and_update_indices(basic_model,d,(150,90),50,150,reset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmGrad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
