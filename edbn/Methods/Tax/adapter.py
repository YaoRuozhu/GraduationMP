import copy
import os
import time
from datetime import datetime

import numpy as np

from Utils.LogFile import LogFile

import tensorflow as tf

ascii_offset = 161

def convert_log(log):
    prefix_size = log.k
    lines = []  # these are all the activity seq
    for name, group in log.contextdata.groupby(log.trace):
        for row in group.iterrows():
            row = row[1]
            line = ""
            for i in range(prefix_size - 1, -1, -1):
                line += chr(int(row["event_Prev%i" % i] + ascii_offset))
            line += chr(int(row["event"] + ascii_offset))
            lines.append(line)
    return lines


def transform_log(log):
    activities = log.values[log.activity]
    X = np.zeros((len(log.contextdata), log.k, len(activities) + 5), dtype=np.float32)
    y_a = np.zeros((len(log.contextdata), len(activities) + 1), dtype=np.float32)
    j = 0
    sum_duration = 0
    count_duration = 0
    for row in log.contextdata.iterrows():
        # if getattr(row[1], "%s_Prev1" % log.activity) != 0: #TODO: ignore events with prefix size of 1
            act = getattr(row[1], log.activity)
            k = 0
            for i in range(log.k -1, -1, -1):
                X[j, log.k - i - 1, getattr(row[1], "%s_Prev%i" % (log.activity, i))] = 1
                X[j, log.k - i - 1, len(activities)] = k

                str_time = getattr(row[1], "%s_Prev%i" % (log.time, i))
                if i == log.k - 1:
                    prev_str = str_time
                else:
                    prev_str = getattr(row[1], "%s_Prev%i" % (log.time, i+1))

                if str_time != 0: #No event
                    try:
                        event_time = time.strptime(str_time, "%Y-%m-%d %H:%M:%S")
                    except ValueError:
                        event_time = time.strptime(str_time, "%Y/%m/%d %H:%M:%S.%f")


                    X[j, log.k - i - 1, getattr(row[1], "%s_Prev%i" % (log.activity, i))] = 1
                    X[j, log.k - i - 1, len(activities)] = k

                    if prev_str != 0:
                        try:
                            prev_time = time.strptime(prev_str, "%Y-%m-%d %H:%M:%S")
                        except ValueError:
                            prev_time = time.strptime(prev_str, "%Y/%m/%d %H:%M:%S.%f")

                        diff_prev_event = datetime.fromtimestamp(time.mktime(event_time)) \
                                          - datetime.fromtimestamp(time.mktime(prev_time))
                        diff = 86400 * diff_prev_event.days + diff_prev_event.seconds
                        sum_duration += diff
                        count_duration += 1
                        X[j, log.k - i - 1, len(activities) + 1] = diff
                        X[j, log.k - i - 1, len(activities) + 2] = 0

                    X[j, log.k - i - 1, len(activities) + 3] = (event_time.tm_hour * 3600 + event_time.tm_min * 60 + event_time.tm_sec) / 86400
                    X[j, log.k - i - 1, len(activities) + 4] = event_time.tm_wday / 7

                    #TODO add Time since previous event / Average time since previous event
                    #TODO add Time since start case / Average time since start case
                    #TODO second in day (starting from midnight) / 86400
                    #TODO day of week / 7
                k += 1
            y_a[j, act] = 1
            j += 1

    #Calculate mean value for all durations
    # avg_duration = sum_duration / count_duration
    # for j in range(j):
    #     for i in range(log.k - 1, -1, -1):
    #         X[j, i, len(activities) + 1] = X[j, i, len(activities) + 1] / avg_duration

    return X, y_a


def train(log, epochs=10, early_stop=42):
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense, BatchNormalization, LSTM
    from tensorflow.keras.models import Model
    from tensorflow.keras.optimizers import Nadam
    """
    transform_log(log)
    input("Waiting")
    
    # lines = convert_log(log)
    lines = convert_log(log)

    maxlen = max(map(lambda x: len(x), lines)) #find maximum line size

    # next lines here to get all possible characters for events and annotate them with numbers
    chars = [chr(i + ascii_offset) for i in range(len(log.values["event"]) + 1)]
    chars.sort()
    target_chars = copy.copy(chars)
    print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))
    char_indices = dict((c, i) for i, c in enumerate(chars))
    indices_char = dict((i, c) for i, c in enumerate(chars))
    target_char_indices = dict((c, i) for i, c in enumerate(target_chars))
    target_indices_char = dict((i, c) for i, c in enumerate(target_chars))

    sentences = []
    next_chars = []

    for line in lines:
        sentences.append(line[:-1])
        next_chars.append(line[-1])
    print('nb sequences:', len(sentences))

    print('Vectorization...')
    num_features = len(chars)+1
    print('num features: {}'.format(num_features))
    X = np.zeros((len(sentences), maxlen, num_features), dtype=np.float32)
    y_a = np.zeros((len(sentences), len(target_chars)), dtype=np.float32)
    for i, sentence in enumerate(sentences):
        leftpad = maxlen-len(sentence)
        for t, char in enumerate(sentence):
            for c in chars:
                if c == char: #this will encode present events to the right places
                    X[i, t+leftpad, char_indices[c]] = 1
                    break
            X[i, t+leftpad, len(chars)] = t+1
        for c in target_chars:
            if c == next_chars[i]:
                y_a[i, target_char_indices[c]] = 1
            else:
                y_a[i, target_char_indices[c]] = 0
        #np.set_printoptions(threshold=np.nan)
    """
    X, y_a = transform_log(log)

    # build the model:
    print('Build model...')
    main_input = Input(shape=(log.k, len(log.values[log.activity])+5), name='main_input')
    # train a 2-layer LSTM with one shared layer
    l1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, dropout=0.2)(main_input) # the shared layer
    b1 = BatchNormalization()(l1)
    l2_1 = LSTM(100, implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, dropout=0.2)(b1) # the layer specialized in activity prediction
    b2_1 = BatchNormalization()(l2_1)

    act_output = Dense(len(log.values[log.activity]) + 1, activation='softmax', kernel_initializer='glorot_uniform', name='act_output')(b2_1)

    model = Model(inputs=[main_input], outputs=[act_output])

    opt = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)

    model.compile(loss={'act_output':'categorical_crossentropy'}, optimizer=opt)
    early_stopping = EarlyStopping(monitor='val_loss', patience=early_stop)
    model_checkpoint = ModelCheckpoint(os.path.join("model", 'model_{epoch:03d}-{val_loss:.2f}.h5'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto')
    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)
    if len(y_a) > 10:
        split = 0.2
    else:
        split = 0

    model.fit(X, {'act_output': y_a}, validation_split=split, verbose=2, callbacks=[early_stopping, lr_reducer], batch_size=log.k, epochs=epochs)

    return model


def update(model, log):
    X, y = transform_log(log)
    if len(X) < 10:
        split = 0
    else:
        split = 0.2
    model.fit(X, {'act_output': y}, validation_split=split, verbose=0,
              batch_size=log.k, epochs=10)
    return model


def test(model, log):
    X, y = transform_log(log)
    predictions = model.predict(X)
    predict_vals = np.argmax(predictions, axis=1)
    predict_probs = predictions[np.arange(predictions.shape[0]), predict_vals]
    expected_vals = np.argmax(y, axis=1)
    expected_probs = predictions[np.arange(predictions.shape[0]), expected_vals]
    result = zip(expected_vals, predict_vals, predict_probs, expected_probs)
    return result

    # lines = convert_log(log)
    #
    # maxlen = max(max(map(lambda x: len(x), lines)), model.input_shape[1])
    #
    # chars = [chr(i + ascii_offset) for i in range(len(log.values["event"]) + 1)]
    # chars.sort()
    # target_chars = copy.copy(chars)
    # print('total chars: {}, target chars: {}'.format(len(chars), len(target_chars)))
    # char_indices = dict((c, i) for i, c in enumerate(chars))
    # indices_char = dict((i, c) for i, c in enumerate(chars))
    # target_char_indices = dict((c, i) for i, c in enumerate(target_chars))
    # target_indices_char = dict((i, c) for i, c in enumerate(target_chars))
    # print(indices_char)
    #
    # # define helper functions
    # def encode(sentence, maxlen=maxlen):
    #     num_features = len(chars)+1
    #     X = np.zeros((1, maxlen, num_features), dtype=np.float32)
    #     leftpad = maxlen-len(sentence)
    #     for t, char in enumerate(sentence):
    #         for c in chars:
    #             if c == char:
    #                 X[0, t+leftpad, char_indices[c]] = 1
    #         X[0, t+leftpad, len(chars)] = t+1
    #     return X
    #
    # results = {}
    # all_results = []
    # for prefix_size in range(1, maxlen):
    #     print(prefix_size)
    #     results[prefix_size] = []
    #     for line in lines:
    #         if prefix_size >= len(line):
    #             continue
    #         cropped_line = ''.join(line[:prefix_size])
    #         ground_truth = ''.join(line[prefix_size:prefix_size + 1])
    #         if ground_truth == "ยก":
    #             continue
    #
    #         enc = encode(cropped_line)
    #         y = model.predict(enc, verbose=0)
    #
    #         predicted_val = np.argmax(y[0])
    #         predicted_prob = y[0][predicted_val]
    #         all_results.append((ground_truth, target_indices_char[predicted_val], predicted_prob))
    #
    # return all_results


def test_and_update(logs, model):
    results = []
    i = 0
    for t in logs:
        print(i, "/", len(logs))
        i += 1
        log = logs[t]["data"]
        results.extend(test(log, model))

        X, y = transform_log(log)
        if len(X) < 10:
            split = 0
        else:
            split = 0.2
        model.fit(X, {'act_output': y}, validation_split=split, verbose=0,
                  batch_size=log.k, epochs=10)

    return results


def test_and_update_retain(test_logs, model, train_log):
    import gc

    train_x, train_y = transform_log(train_log)

    results = []
    i = 0
    for t in test_logs:
        print(i, "/", len(test_logs))
        i += 1
        test_log = test_logs[t]["data"]
        results.extend(test(test_log, model))
        test_x, test_y = transform_log(test_log)
        train_x = np.concatenate((train_x, test_x))
        train_y = np.concatenate((train_y, test_y))
        model.fit(train_x, {'act_output': train_y},
                  validation_split=0.2,
                  verbose=0,
                  batch_size=train_log.k,
                  epochs=1)
        gc.collect() # Avoid that too many batches lead to full memory
    return results

if __name__ == "__main__":
    data = "../../Data/BPIC12W.csv"
    # data = "../../Data/Helpdesk.csv"
    # data = "../../Data/Taymouri_bpi_12_w.csv"
    case_attr = "case"
    act_attr = "event"
    k = 15

    logfile = LogFile(data, ",", 0, None, None, case_attr,
                      activity_attr=act_attr, convert=False, k=10)
    logfile.convert2int()
    logfile.filter_case_length(5)
    # logfile.k = min(k, max(logfile.data.groupby(logfile.trace).size()))

    logfile.create_k_context()
    train_log, test_log = logfile.splitTrainTest(70, case=False, method="test-train")

    model = train(train_log, epochs=100, early_stop=10)
    # model.save("tmp.h5")
    # from keras.models import load_model
    # test(test_log, load_model("tmp.h5"))
    test(test_log, model)
